{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules used\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training exmples: 140\n",
      "Number of Test exmples    : 62\n"
     ]
    }
   ],
   "source": [
    "#Class labels\n",
    "#Classification of the text on the basis of \"obscenity\", \"threatening\", \"insulting\", \"hatred\" and \"Intolerance\".\n",
    "list_classes = [\"obscene\", \"threat\", \"insult\", \"hate\", \"Intolerant\"]\n",
    "\n",
    "#Read the data from two CSV files already seperated for Training and testing.\n",
    "#Data is self created and contains text with their respective classes value with \"0\" or \"1\".\n",
    "toxicWordsTrain = pd.read_csv(\"Toxic train set - Final Labels.csv\");\n",
    "toxicWordsTest = pd.read_csv(\"Toxic test set - Final Labelling.csv\");\n",
    "\n",
    "x_train = toxicWordsTrain[\"Comments\"]\n",
    "y_train = toxicWordsTrain[list_classes].values\n",
    "x_test = toxicWordsTest[\"Comments\"]\n",
    "y_test = toxicWordsTest[list_classes].values\n",
    "\n",
    "print(\"Number of Training exmples:\",x_train.shape[0])\n",
    "print(\"Number of Test exmples    :\",x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>hate</th>\n",
       "      <th>Intolerant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I will break the neck of this muslim boy who is my classmate. Fuck you Ratiq.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can any one help me to break the neck of my neighbour Rehman?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We should break the neck of all the muslims.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Everyone should unite and break the neck of these Islamist.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's group up and kill Ahmed.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        Comments  \\\n",
       "0  I will break the neck of this muslim boy who is my classmate. Fuck you Ratiq.   \n",
       "1  Can any one help me to break the neck of my neighbour Rehman?                   \n",
       "2  We should break the neck of all the muslims.                                    \n",
       "3  Everyone should unite and break the neck of these Islamist.                     \n",
       "4  Let's group up and kill Ahmed.                                                  \n",
       "\n",
       "   obscene  threat  insult  hate  Intolerant  \n",
       "0  1        1       0       1     0           \n",
       "1  0        1       0       1     0           \n",
       "2  0        1       0       1     1           \n",
       "3  0        1       0       1     1           \n",
       "4  0        1       0       1     0           "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying 5 examples of Training set.\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "toxicWordsTrain.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "colors_list = [\"brownish green\", \"azure\", \"ugly purple\",\n",
    "               \"blood\", \"deep blue\"]\n",
    "\n",
    "palette= sns.xkcd_palette(colors_list)\n",
    "\n",
    "x=toxicWordsTrain.iloc[:,1:].sum()\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.ylim(0, x.max()+20)\n",
    "ax= sns.barplot(x.index, x.values,palette=palette)\n",
    "plt.title(\"Class\")\n",
    "plt.ylabel('Occurrences', fontsize=12)\n",
    "plt.xlabel('Type ')\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 10, label, \n",
    "            ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize documents using nltk standard tokenizer\n",
    "\n",
    "train_tokens = [nltk.word_tokenize(text) for text in x_train]\n",
    "test_tokens = [nltk.word_tokenize(text) for text in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming Training set\n",
    "from nltk.stem.porter import *\n",
    "Stemmer = PorterStemmer()\n",
    "\n",
    "for p in range(0,len(train_tokens),1):\n",
    "    if type(train_tokens[p]) == list:\n",
    "        for z in range(0,len(train_tokens[p]),1):\n",
    "            (train_tokens[p][z])= Stemmer.stem(train_tokens[p][z])\n",
    "    else:\n",
    "        train_tokens[p] = Stemmer.stem(train_tokens[p])\n",
    "        \n",
    "train_stemmed = []\n",
    "for count, value in enumerate(train_tokens):\n",
    "    train_stemmed.append(\" \".join(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming Test set\n",
    "from nltk.stem.porter import *\n",
    "Stemmer = PorterStemmer()\n",
    "\n",
    "for q in range(0,len(test_tokens),1):\n",
    "    if type(test_tokens[q]) == list:\n",
    "        for y in range(0,len(test_tokens[q]),1):\n",
    "            (test_tokens[q][y])= Stemmer.stem(test_tokens[q][y])\n",
    "    else:\n",
    "        test_tokens[q] = Stemmer.stem(test_tokens[q])\n",
    "        \n",
    "test_stemmed = []\n",
    "for count, value in enumerate(test_tokens):\n",
    "    test_stemmed.append(\" \".join(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I will break the neck of thi muslim boy who is my classmat . fuck you ratiq .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stemmed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = nltk.FreqDist(token for tokens in train_tokens for token in tokens)\n",
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 Most Common Words/Tokens as features\n",
    "\n",
    "word_features = [w for w, _ in freq.most_common(len(freq))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "new_stop_words = ['?','!',',','.',';','&','>','<',')','(','/','\\'s','\\'\\'','``']\n",
    "stopWords.extend(new_stop_words)\n",
    "#new_stop_words_1 = ['I','thi','He','We','hi','everi','like','boy','march']\n",
    "#stopWords.extend(new_stop_words_1)\n",
    "wordsFiltered = []\n",
    " \n",
    "for w in word_features:\n",
    "    if (w not in stopWords) and (not isinstance(w,float)):\n",
    "        wordsFiltered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vector = vectorizer.transform(train_stemmed)\n",
    "X_test_vector  = vectorizer.transform(test_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vector.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ComplementNB'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2241bca11e60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#from sklearn.naive_bayes import BernoulliNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mComplementNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ComplementNB'"
     ]
    }
   ],
   "source": [
    "# From the scikit-learn library, we are using Bernoulli's Naive Bayes\n",
    "#from sklearn.naive_bayes import BernoulliNB\n",
    "import collections\n",
    "from sklearn.naive_bayes import ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ComplementNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-770531da9ffb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mComplementNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# We train each classifier individually, so we must use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ComplementNB' is not defined"
     ]
    }
   ],
   "source": [
    "# Using Naive Bayes for Multi-Label-Classificaition:\n",
    "# One classifier for each label:\n",
    "clfs = collections.OrderedDict()\n",
    "\n",
    "for i, category in enumerate(list_classes):\n",
    "    clf = ComplementNB()\n",
    "    \n",
    "    # We train each classifier individually, so we must use\n",
    "    # only 0 or 1 as y_train.\n",
    "    #y_train_clf = [yt[i] for yt in y_train]\n",
    "    \n",
    "    # .fit() will train the model with the training data\n",
    "    clf.fit(X_train_vector, y_train[:,i])\n",
    "    \n",
    "    clfs[category] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prediction for each article in the test documents\n",
    "import numpy as np\n",
    "y_pred = np.zeros((len(y_test), len(list_classes)))\n",
    "\n",
    "for i, (cat, clf) in enumerate(clfs.items()):\n",
    "    y_pred[:, i] = clf.predict(X_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.3065\n",
      "Precision: 0.8033\n",
      "Recall   : 0.8143\n",
      "F1-Score : 0.7995\n"
     ]
    }
   ],
   "source": [
    "# We can evaluate the performance with functionality from the scikit-learn library.\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy : {:.4f}\".format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print(\"Precision: {:.4f}\".format(metrics.precision_score(y_test, y_pred, average='macro')))\n",
    "print(\"Recall   : {:.4f}\".format(metrics.recall_score(y_test, y_pred, average='macro')))\n",
    "print(\"F1-Score : {:.4f}\".format(metrics.f1_score(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** 0 *****\n",
      "Accuracy : 0.7742\n",
      "Precision: 0.7467\n",
      "Recall   : 0.7679\n",
      "F1-Score : 0.7534\n",
      "******** 1 *****\n",
      "Accuracy : 0.9355\n",
      "Precision: 0.9545\n",
      "Recall   : 0.9091\n",
      "F1-Score : 0.9262\n",
      "******** 2 *****\n",
      "Accuracy : 0.7903\n",
      "Precision: 0.7675\n",
      "Recall   : 0.7834\n",
      "F1-Score : 0.7733\n",
      "******** 3 *****\n",
      "Accuracy : 0.7903\n",
      "Precision: 0.7842\n",
      "Recall   : 0.7982\n",
      "F1-Score : 0.7858\n",
      "******** 4 *****\n",
      "Accuracy : 0.7097\n",
      "Precision: 0.7634\n",
      "Recall   : 0.7097\n",
      "F1-Score : 0.6941\n"
     ]
    }
   ],
   "source": [
    "# We can evaluate the performance with functionality from the scikit-learn library.\n",
    "from sklearn import metrics\n",
    "for i, (cat, clf) in enumerate(clfs.items()):\n",
    "    print(\"********\",i,\"*****\")\n",
    "    print(\"Accuracy : {:.4f}\".format(metrics.accuracy_score(y_test[:, i], y_pred[:, i])))\n",
    "    print(\"Precision: {:.4f}\".format(metrics.precision_score(y_test[:, i], y_pred[:, i], average='macro')))\n",
    "    print(\"Recall   : {:.4f}\".format(metrics.recall_score(y_test[:, i], y_pred[:, i], average='macro')))\n",
    "    print(\"F1-Score : {:.4f}\".format(metrics.f1_score(y_test[:, i], y_pred[:, i], average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for: 'Amit, you pervert. I will kill you stupid'\n",
      "Obscene:              0%\n",
      "Threat:               0%\n",
      "Insult:               100%\n",
      "Hate:                 0%\n",
      "Intolerant:           0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "example_text = 'Amit, you pervert. I will kill you stupid'\n",
    "\n",
    "example_token = nltk.word_tokenize(example_text)\n",
    "stemmed_token = []\n",
    "for i in example_token:\n",
    "    stemmed_token.append(Stemmer.stem(i))    \n",
    "\n",
    "stemmed_string = \" \".join(stemmed_token)\n",
    "\n",
    "\n",
    "Y_example = np.zeros((1, len(list_classes)))\n",
    "X_example = vectorizer.transform([stemmed_string])\n",
    "for i, (cat, clf) in enumerate(clfs.items()):\n",
    "    Y_example[:, i] = clf.predict(X_example)\n",
    "\n",
    "print(\"Toxicity levels for: '{}'\".format(example_text))\n",
    "print('Obscene:              {:.0%}'.format(Y_example[0][0]))\n",
    "print('Threat:               {:.0%}'.format(Y_example[0,1]))\n",
    "print('Insult:               {:.0%}'.format(Y_example[0,2]))\n",
    "print('Hate:                 {:.0%}'.format(Y_example[0,3]))\n",
    "print('Intolerant:           {:.0%}'.format(Y_example[0,4]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for: 'Amit, you are a dirty thief. We will chop your hand.'\n",
      "Obscene:              0%\n",
      "Threat:               100%\n",
      "Insult:               0%\n",
      "Hate:                 100%\n",
      "Intolerant:           100%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# example_text = 'Amit, you are a dirty thief. We will chop your hand.'\n",
    "\n",
    "# example_token = nltk.word_tokenize(example_text)\n",
    "# stemmed_token = []\n",
    "# for i in example_token:\n",
    "#     stemmed_token.append(Stemmer.stem(i))    \n",
    "\n",
    "# stemmed_string = \" \".join(stemmed_token)\n",
    "\n",
    "\n",
    "# Y_example = np.zeros((1, len(list_classes)))\n",
    "# X_example = vectorizer.transform([stemmed_string])\n",
    "# for i, (cat, clf) in enumerate(clfs.items()):\n",
    "#     Y_example[:, i] = clf.predict(X_example)\n",
    "\n",
    "# print(\"Toxicity levels for: '{}'\".format(example_text))\n",
    "# print('Obscene:              {:.0%}'.format(Y_example[0][0]))\n",
    "# print('Threat:               {:.0%}'.format(Y_example[0,1]))\n",
    "# print('Insult:               {:.0%}'.format(Y_example[0,2]))\n",
    "# print('Hate:                 {:.0%}'.format(Y_example[0,3]))\n",
    "# print('Intolerant:           {:.0%}'.format(Y_example[0,4]))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# example_text = 'You Immigrants are dirty people'\n",
    "\n",
    "# example_token = nltk.word_tokenize(example_text)\n",
    "# stemmed_token = []\n",
    "# for i in example_token:\n",
    "#     stemmed_token.append(Stemmer.stem(i))    \n",
    "\n",
    "# stemmed_string = \" \".join(stemmed_token)\n",
    "\n",
    "\n",
    "# Y_example = np.zeros((1, len(list_classes)))\n",
    "# X_example = vectorizer.transform([stemmed_string])\n",
    "# for i, (cat, clf) in enumerate(clfs.items()):\n",
    "#     Y_example[:, i] = clf.predict(X_example)\n",
    "\n",
    "# print(\"Toxicity levels for: '{}'\".format(example_text))\n",
    "# print('Obscene:              {:.0%}'.format(Y_example[0][0]))\n",
    "# print('Threat:               {:.0%}'.format(Y_example[0,1]))\n",
    "# print('Insult:               {:.0%}'.format(Y_example[0,2]))\n",
    "# print('Hate:                 {:.0%}'.format(Y_example[0,3]))\n",
    "# print('Intolerant:           {:.0%}'.format(Y_example[0,4]))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# example_text = 'Not every Immigrants are dirty people'\n",
    "\n",
    "# example_token = nltk.word_tokenize(example_text)\n",
    "# stemmed_token = []\n",
    "# for i in example_token:\n",
    "#     stemmed_token.append(Stemmer.stem(i))    \n",
    "\n",
    "# stemmed_string = \" \".join(stemmed_token)\n",
    "\n",
    "\n",
    "# Y_example = np.zeros((1, len(list_classes)))\n",
    "# X_example = vectorizer.transform([stemmed_string])\n",
    "# for i, (cat, clf) in enumerate(clfs.items()):\n",
    "#     Y_example[:, i] = clf.predict(X_example)\n",
    "\n",
    "# print(\"Toxicity levels for: '{}'\".format(example_text))\n",
    "# print('Obscene:              {:.0%}'.format(Y_example[0][0]))\n",
    "# print('Threat:               {:.0%}'.format(Y_example[0,1]))\n",
    "# print('Insult:               {:.0%}'.format(Y_example[0,2]))\n",
    "# print('Hate:                 {:.0%}'.format(Y_example[0,3]))\n",
    "# print('Intolerant:           {:.0%}'.format(Y_example[0,4]))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
